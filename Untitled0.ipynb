{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJNfE/aw7q4lkeIbporV3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlbfalchetti/Python/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qSoj94Yit8u"
      },
      "outputs": [],
      "source": [
        "def build_generator(batch_size, seq_length, load_generator_function, n_classes=1, n_samples=None, sequence_templates=None, batch_normalize_pwm=False, anneal_pwm_logits=False, validation_sample_mode='max', supply_inputs=False) :\n",
        "\n",
        "\tsequence_class_input, sequence_class = None, None\n",
        "\t#Seed class input for all dense/embedding layers\n",
        "\tif not supply_inputs :\n",
        "\t\tsequence_class_input = Input(tensor=K.ones((batch_size, 1)), dtype='int32', name='sequence_class_seed')\n",
        "\t\tsequence_class = Lambda(lambda inp: K.cast(K.round(inp * K.random_uniform((batch_size, 1), minval=-0.4999, maxval=n_classes-0.5001)), dtype='int32'), name='lambda_rand_sequence_class')(sequence_class_input)\n",
        "\telse :\n",
        "\t\tsequence_class_input = Input(batch_shape=(batch_size, 1), dtype='int32', name='sequence_class_seed')\n",
        "\t\tsequence_class = Lambda(lambda inp: inp, name='lambda_rand_sequence_class')(sequence_class_input)\n",
        "\n",
        "\t#Get generated policy pwm logits (non-masked)\n",
        "\tgenerator_inputs, [raw_logits_1, raw_logits_2], extra_outputs = load_generator_function(batch_size, sequence_class, n_classes=n_classes, seq_length=seq_length, supply_inputs=supply_inputs)\n",
        "\n",
        "\treshape_layer = Reshape((seq_length, 4, 1))\n",
        "\t\n",
        "\tonehot_template_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='template_dense')\n",
        "\tonehot_mask_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='ones', name='mask_dense')\n",
        "\t\n",
        "\tonehot_template = reshape_layer(onehot_template_dense(sequence_class))\n",
        "\tonehot_mask = reshape_layer(onehot_mask_dense(sequence_class))\n",
        "\n",
        "\t#Initialize Templating and Masking Lambda layer\n",
        "\tmasking_layer = Lambda(mask_pwm, output_shape = (seq_length, 4, 1), name='masking_layer')\n",
        "\n",
        "\t#Batch Normalize PWM Logits\n",
        "\tif batch_normalize_pwm :\n",
        "\t\traw_logit_batch_norm = BatchNormalization(name='policy_raw_logit_batch_norm')\n",
        "\t\traw_logits_1 = raw_logit_batch_norm(raw_logits_1)\n",
        "\t\traw_logits_2 = raw_logit_batch_norm(raw_logits_2)\n",
        "\t\n",
        "\t#Add Template and Multiply Mask\n",
        "\tpwm_logits_1 = masking_layer([raw_logits_1, onehot_template, onehot_mask])\n",
        "\tpwm_logits_2 = masking_layer([raw_logits_2, onehot_template, onehot_mask])\n",
        "\t\n",
        "\t#Compute PWMs (Nucleotide-wise Softmax)\n",
        "\tpwm_1 = Softmax(axis=-2, name='pwm_1')(pwm_logits_1)\n",
        "\tpwm_2 = Softmax(axis=-2, name='pwm_2')(pwm_logits_2)\n",
        "\t\n",
        "\tanneal_temp = None\n",
        "\tif anneal_pwm_logits :\n",
        "\t\tanneal_temp = K.variable(1.0)\n",
        "\t\t\n",
        "\t\tinterpolated_pwm_1 = Lambda(lambda x: (1. - anneal_temp) * x + anneal_temp * 0.25)(pwm_1)\n",
        "\t\tinterpolated_pwm_2 = Lambda(lambda x: (1. - anneal_temp) * x + anneal_temp * 0.25)(pwm_2)\n",
        "\t\t\n",
        "\t\tpwm_logits_1 = Lambda(lambda x: K.log(x / (1. - x)))(interpolated_pwm_1)\n",
        "\t\tpwm_logits_2 = Lambda(lambda x: K.log(x / (1. - x)))(interpolated_pwm_2)\n",
        "\t\n",
        "\t#Sample proper One-hot coded sequences from PWMs\n",
        "\tsampled_pwm_1, sampled_pwm_2, sampled_onehot_mask = None, None, None\n",
        "\n",
        "\tsample_func = sample_pwm\n",
        "\tif validation_sample_mode == 'sample' :\n",
        "\t\tsample_func = sample_pwm_only\n",
        "\n",
        "\t#Optionally tile each PWM to sample from and create sample axis\n",
        "\tif use_samples :\n",
        "\t\tpwm_logits_upsampled_1 = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]))(pwm_logits_1)\n",
        "\t\tpwm_logits_upsampled_2 = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]))(pwm_logits_2)\n",
        "\t\tsampled_onehot_mask = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]))(onehot_mask)\n",
        "\n",
        "\t\tsampled_pwm_1 = Lambda(sample_func, name='pwm_sampler_1')(pwm_logits_upsampled_1)\n",
        "\t\t#sampled_pwm_1 = Lambda(lambda x: K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)))(sampled_pwm_1)\n",
        "\t\tsampled_pwm_1 = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)), (1, 0, 2, 3, 4)))(sampled_pwm_1)\n",
        "\n",
        "\t\tsampled_pwm_2 = Lambda(sample_func, name='pwm_sampler_2')(pwm_logits_upsampled_2)\n",
        "\t\t#sampled_pwm_2 = Lambda(lambda x: K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)))(sampled_pwm_2)\n",
        "\t\tsampled_pwm_2 = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)), (1, 0, 2, 3, 4)))(sampled_pwm_2)\n",
        "\n",
        "\t\t\n",
        "\t\t#sampled_onehot_mask = Lambda(lambda x: K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)), (1, 0, 2, 3, 4))(sampled_onehot_mask)\n",
        "\t\tsampled_onehot_mask = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)), (1, 0, 2, 3, 4)))(sampled_onehot_mask)\n",
        "\n",
        "\telse :\n",
        "\t\tsampled_pwm_1 = Lambda(sample_func, name='pwm_sampler_1')(pwm_logits_1)\n",
        "\t\tsampled_pwm_2 = Lambda(sample_func, name='pwm_sampler_2')(pwm_logits_2)\n",
        "\t\tsampled_onehot_mask = onehot_mask\n",
        "\t\n",
        "\t\n",
        "\tgenerator_model = Model(\n",
        "\t\tinputs=[\n",
        "\t\t\tsequence_class_input\n",
        "\t\t] + generator_inputs,\n",
        "\t\toutputs=[\n",
        "\t\t\tsequence_class,\n",
        "\t\t\tpwm_logits_1,\n",
        "\t\t\tpwm_logits_2,\n",
        "\t\t\tpwm_1,\n",
        "\t\t\tpwm_2,\n",
        "\t\t\tsampled_pwm_1,\n",
        "\t\t\tsampled_pwm_2\n",
        "\n",
        "\t\t\t,onehot_mask\n",
        "\t\t\t,sampled_onehot_mask\n",
        "\t\t] + extra_outputs\n",
        "\t)\n",
        "\n",
        "\tif sequence_templates is not None :\n",
        "\t\tinitialize_sequence_templates(generator_model, sequence_templates)\n",
        "\n",
        "\t#Lock all generator layers except policy layers\n",
        "\tfor generator_layer in generator_model.layers :\n",
        "\t\tgenerator_layer.trainable = False\n",
        "\t\t\n",
        "\t\tif 'policy' in generator_layer.name :\n",
        "\t\t\tgenerator_layer.trainable = True\n",
        "\n",
        "\tif anneal_pwm_logits :\n",
        "\t\treturn 'genesis_generator', generator_model, anneal_temp\n",
        "\treturn 'genesis_generator', generator_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install isolearn\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
        "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
        "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "import keras.losses\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "import isolearn.keras as iso\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w61L6yPrpsFS",
        "outputId": "b32fc695-d96b-4de6-ca5b-31a60bdda6c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: isolearn in /usr/local/lib/python3.8/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of PWMs to generate per objective\n",
        "batch_size = 36\n",
        "#Number of One-hot sequences to sample from the PWM at each grad step\n",
        "n_samples = 10\n",
        "#Number of epochs per objective to optimize\n",
        "n_epochs = 50\n",
        "#Number of steps (grad updates) per epoch\n",
        "steps_per_epoch = 500\n",
        "\n",
        "n_classes = 1\n",
        "\n",
        "supply_inputs = False\n",
        "\n",
        "# Then: True\n",
        "\n",
        "sequence_class_input = Input(tensor = K.ones((batch_size, 1)), dtype = \"float32\", name = \"sequence_class_seed\")\n",
        "sequence_class = Lambda(lambda inp: K.cast(K.round(inp * K.random_uniform((batch_size, 1), minval = -0.4999, maxval = n_classes - 0.5001)), dtype = \"float32\"), name = \"lambda_random_sequence_class\")(sequence_class_input)"
      ],
      "metadata": {
        "id": "l0oSltVkwo6f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_class_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlHBI1wdwzO9",
        "outputId": "8fcbca89-4f95-4945-f140-a43efc192e3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(36, 1) dtype=float32 (created by layer 'sequence_class_seed')>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayv_DIQtw1b-",
        "outputId": "6ad81b12-85a9-49fe-aef2-6715806c42cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(36, 1) dtype=float32 (created by layer 'lambda_random_sequence_class')>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_templates = [\n",
        "    'TCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNANTAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAATTGTTCGTTGGTCGGCTTGAGTGCGTGTGTCTCGTTTAGATGCTGCGCCTAACCCTAAGCAGATTCTTCATGCAATTG',\n",
        "    'TCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNANTAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAATTGTTCGTTGGTCGGCTTGAGTGCGTGTGTCTCGTTTAGATGCTGCGCCTAACCCTAAGCAGATTCTTCATGCAATTG',\n",
        "    'TCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNANTAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAATTGTTCGTTGGTCGGCTTGAGTGCGTGTGTCTCGTTTAGATGCTGCGCCTAACCCTAAGCAGATTCTTCATGCAATTG',\n",
        "    'TCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNANTAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAATTGTTCGTTGGTCGGCTTGAGTGCGTGTGTCTCGTTTAGATGCTGCGCCTAACCCTAAGCAGATTCTTCATGCAATTG',\n",
        "    'TCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNANTAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAATTGTTCGTTGGTCGGCTTGAGTGCGTGTGTCTCGTTTAGATGCTGCGCCTAACCCTAAGCAGATTCTTCATGCAATTG'\n",
        "]\n",
        "\n",
        "library_contexts = [\n",
        "    'simple',\n",
        "    'simple',\n",
        "    'simple',\n",
        "    'simple',\n",
        "    'simple'\n",
        "]\n",
        "\n",
        "target_isos = [\n",
        "    0.05,\n",
        "    0.25,\n",
        "    0.5,\n",
        "    0.75,\n",
        "    1.0\n",
        "]\n",
        "\n",
        "margin_similarities = [\n",
        "    0.3,\n",
        "    0.3,\n",
        "    0.3,\n",
        "    0.3,\n",
        "    0.5\n",
        "]"
      ],
      "metadata": {
        "id": "NRFok6DisJZV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_templates = [\n",
        "    'TCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNANTAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAATTGTTCGTTGGTCGGCTTGAGTGCGTGTGTCTCGTTTAGATGCTGCGCCTAACCCTAAGCAGATTCTTCATGCAATTG',\n",
        "]\n",
        "\n",
        "library_contexts = [\n",
        "    'simple',\n",
        "]\n",
        "\n",
        "target_isos = [\n",
        "    1.0\n",
        "]\n",
        "\n",
        "margin_similarities = [\n",
        "    0.5\n",
        "]"
      ],
      "metadata": {
        "id": "ijQUwjxzgDkl"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
        "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
        "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "import keras.losses\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import isolearn.keras as iso\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#GENESIS Generator Model definitions\n",
        "def load_generator_network(batch_size, sequence_class, n_classes=1, seq_length=205, supply_inputs=False) :\n",
        "\n",
        "\tsequence_class_onehots = np.eye(n_classes)\n",
        "\n",
        "\t#Generator network parameters\n",
        "\tlatent_size = 100\n",
        "\t\n",
        "\t#Generator inputs\n",
        "\tlatent_input_1, latent_input_2, latent_input_1_out, latent_input_2_out = None, None, None, None\n",
        "\tif not supply_inputs :\n",
        "\t\tlatent_input_1 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_1')\n",
        "\t\tlatent_input_2 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_2')\n",
        "\t\tlatent_input_1_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_1')(latent_input_1)\n",
        "\t\tlatent_input_2_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_2')(latent_input_2)\n",
        "\telse :\n",
        "\t\tlatent_input_1 = Input(batch_shape=K.ones(batch_size, latent_size), name='noise_input_1')\n",
        "\t\tlatent_input_2 = Input(batch_shape=K.ones(batch_size, latent_size), name='noise_input_2')\n",
        "\t\tlatent_input_1_out = Lambda(lambda inp: inp, name='lambda_rand_input_1')(latent_input_1)\n",
        "\t\tlatent_input_2_out = Lambda(lambda inp: inp, name='lambda_rand_input_2')(latent_input_2)\n",
        "\t\n",
        "\tclass_embedding = Lambda(lambda x: K.gather(K.constant(sequence_class_onehots), K.cast(x[:, 0], dtype='int32')))(sequence_class)\n",
        "\n",
        "\tseed_input_1 = Concatenate(axis=-1)([latent_input_1_out, class_embedding])\n",
        "\tseed_input_2 = Concatenate(axis=-1)([latent_input_2_out, class_embedding])\n",
        "\t\n",
        "\t\n",
        "\t#Policy network definition\n",
        "\tpolicy_dense_1 = Dense(21 * 384, activation='relu', kernel_initializer='glorot_uniform', name='policy_dense_1')\n",
        "\t\n",
        "\tpolicy_dense_1_reshape = Reshape((21, 1, 384))\n",
        "\t\n",
        "\tpolicy_deconv_0 = Conv2DTranspose(256, (7, 1), strides=(2, 1), padding='valid', activation='linear', kernel_initializer='glorot_normal', name='policy_deconv_0')\n",
        "\t\n",
        "\tpolicy_deconv_1 = Conv2DTranspose(192, (8, 1), strides=(2, 1), padding='valid', activation='linear', kernel_initializer='glorot_normal', name='policy_deconv_1')\n",
        "\t\n",
        "\tpolicy_deconv_2 = Conv2DTranspose(128, (7, 1), strides=(2, 1), padding='valid', activation='linear', kernel_initializer='glorot_normal', name='policy_deconv_2')\n",
        "\t\n",
        "\tpolicy_conv_3 = Conv2D(128, (8, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_conv_3')\n",
        "\n",
        "\tpolicy_conv_4 = Conv2D(64, (8, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_conv_4')\n",
        "\n",
        "\tpolicy_conv_5 = Conv2D(4, (8, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_conv_5')\n",
        "\n",
        "\t#policy_deconv_3 = Conv2DTranspose(4, (7, 1), strides=(1, 1), padding='valid', activation='linear', kernel_initializer='glorot_normal', name='policy_deconv_3')\n",
        "\t\n",
        "\tbatch_norm_0 = BatchNormalization(name='policy_batch_norm_0')\n",
        "\trelu_0 = Lambda(lambda x: K.relu(x))\n",
        "\tbatch_norm_1 = BatchNormalization(name='policy_batch_norm_1')\n",
        "\trelu_1 = Lambda(lambda x: K.relu(x))\n",
        "\tbatch_norm_2 = BatchNormalization(name='policy_batch_norm_2')\n",
        "\trelu_2 = Lambda(lambda x: K.relu(x))\n",
        "\n",
        "\tbatch_norm_3 = BatchNormalization(name='policy_batch_norm_3')\n",
        "\trelu_3 = Lambda(lambda x: K.relu(x))\n",
        "\n",
        "\tbatch_norm_4 = BatchNormalization(name='policy_batch_norm_4')\n",
        "\trelu_4 = Lambda(lambda x: K.relu(x))\n",
        "\n",
        "\tpolicy_out_1 = Reshape((seq_length, 4, 1))(policy_conv_5(relu_4(batch_norm_4(policy_conv_4(relu_3(batch_norm_3(policy_conv_3(relu_2(batch_norm_2(policy_deconv_2(relu_1(batch_norm_1(policy_deconv_1(relu_0(batch_norm_0(policy_deconv_0(policy_dense_1_reshape(policy_dense_1(seed_input_1)))))))))))))))))))\n",
        "\tpolicy_out_2 = Reshape((seq_length, 4, 1))(policy_conv_5(relu_4(batch_norm_4(policy_conv_4(relu_3(batch_norm_3(policy_conv_3(relu_2(batch_norm_2(policy_deconv_2(relu_1(batch_norm_1(policy_deconv_1(relu_0(batch_norm_0(policy_deconv_0(policy_dense_1_reshape(policy_dense_1(seed_input_2)))))))))))))))))))\n",
        "\t\n",
        "\treturn [latent_input_1, latent_input_2], [policy_out_1, policy_out_2], []"
      ],
      "metadata": {
        "id": "2SJ6W759uA1A"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = len(sequence_templates[0])\n",
        "\n",
        "load_generator_function = load_generator_network\n",
        "\n",
        "#Get generated policy pwm logits (non-masked)\n",
        "generator_inputs, [raw_logits_1, raw_logits_2], extra_outputs = load_generator_function(batch_size, sequence_class, n_classes = n_classes, seq_length = seq_length, supply_inputs = supply_inputs)"
      ],
      "metadata": {
        "id": "orAzD_vXk0Wj"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9lK2c_qufZ0",
        "outputId": "48171ee6-23ff-4412-c184-f28e7ef01814"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(36, 100) dtype=float32 (created by layer 'noise_input_1')>,\n",
              " <KerasTensor: shape=(36, 100) dtype=float32 (created by layer 'noise_input_2')>]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_logits_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbNyBx96uk09",
        "outputId": "4645b2af-73eb-4e80-aa0b-638f75651e70"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(36, 205, 4, 1) dtype=float32 (created by layer 'reshape_5')>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_logits_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K58pLKAsuo5s",
        "outputId": "852788ad-f7e1-4b6f-dce9-33ea1180afae"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(36, 205, 4, 1) dtype=float32 (created by layer 'reshape_6')>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3uvhziLurl4",
        "outputId": "f32ce361-2a99-4ed2-9fe0-c0c591ce5767"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshape_layer = Reshape((seq_length, 4, 1))\n",
        "\t\n",
        "onehot_template_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer = 'zeros', name = 'template_dense')\n",
        "onehot_mask_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer = 'ones', name = 'mask_dense')\n",
        "\t\n",
        "onehot_template = reshape_layer(onehot_template_dense(sequence_class))\n",
        "onehot_mask = reshape_layer(onehot_mask_dense(sequence_class))"
      ],
      "metadata": {
        "id": "edD0ZJa5uzpv"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keV05h62vHgg",
        "outputId": "4ffed9cd-cfbf-443e-93ab-d39f19b5df60"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(36, 205, 4, 1) dtype=float32 (created by layer 'reshape_7')>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJyQcphYvO21",
        "outputId": "f5d908e8-3f09-41f2-cdc3-93127208e0e6"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(36, 205, 4, 1) dtype=float32 (created by layer 'reshape_7')>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_pwm(inputs) :\n",
        "\tpwm, onehot_template, onehot_mask = inputs\n",
        "\n",
        "\treturn pwm * onehot_mask + onehot_template\n",
        "\n",
        "# Initialize templating and masking lambda layer\n",
        "masking_layer = Lambda(mask_pwm, output_shape = (seq_length, 4, 1), name = \"masking_layer\")"
      ],
      "metadata": {
        "id": "LnD1iPkjvXSt"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_normalize_pwm = False\n",
        "\n",
        "# Batch Normalize PWM Logits\n",
        "if batch_normalize_pwm :\n",
        "  raw_logit_batch_norm = BatchNormalization(name = \"policy_raw_logit_batch_norm\")\n",
        "  raw_logits_1 = raw_logit_batch_norm(raw_logits_1)\n",
        "  raw_logits_2 = raw_logit_batch_norm(raw_logits_2)"
      ],
      "metadata": {
        "id": "qwAGjH0XwsHS"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Template and Multiply Mask\n",
        "pwm_logits_1 = masking_layer([raw_logits_1, onehot_template, onehot_mask])\n",
        "pwm_logits_2 = masking_layer([raw_logits_2, onehot_template, onehot_mask])"
      ],
      "metadata": {
        "id": "rVH9UCXox9Ky"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute PWMs (Nucleotide-wise Softmax)\n",
        "pwm_1 = Softmax(axis = -2, name = 'pwm_1')(pwm_logits_1)\n",
        "pwm_2 = Softmax(axis = -2, name = 'pwm_2')(pwm_logits_2)\t"
      ],
      "metadata": {
        "id": "kb6hZCto0o9d"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample proper One-hot coded sequences from PWMs\n",
        "sampled_pwm_1, sampled_pwm_2, sampled_onehot_mask = None, None, None\n",
        "\n",
        "def st_sampled_softmax(logits):\n",
        "\twith ops.name_scope(\"STSampledSoftmax\") as namescope :\n",
        "\t\tnt_probs = tf.nn.softmax(logits)\n",
        "\t\tonehot_dim = logits.get_shape().as_list()[1]\n",
        "\t\tsampled_onehot = tf.one_hot(tf.squeeze(tf.random.categorical(logits, 1), 1), onehot_dim, 1.0, 0.0)\n",
        "\t\twith tf.Graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
        "\t\t\treturn tf.math.ceil(sampled_onehot * nt_probs)\n",
        "\n",
        "def st_hardmax_softmax(logits):\n",
        "\twith ops.name_scope(\"STHardmaxSoftmax\") as namescope :\n",
        "\t\tnt_probs = tf.nn.softmax(logits)\n",
        "\t\tonehot_dim = logits.get_shape().as_list()[1]\n",
        "\t\tsampled_onehot = tf.one_hot(tf.argmax(nt_probs, 1), onehot_dim, 1.0, 0.0)\n",
        "\t\twith tf.Graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
        "\t\t\treturn tf.math.ceil(sampled_onehot * nt_probs)\n",
        "\n",
        "#@ops.RegisterGradient(\"STMul\")\n",
        "#def st_mul(op, grad):\n",
        "#\treturn [grad, grad]\n",
        "\n",
        "def sample_pwm(pwm_logits) :\n",
        "\tn_sequences = K.shape(pwm_logits)[0]\n",
        "\tseq_length = K.shape(pwm_logits)[1]\n",
        "\t\n",
        "\tflat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
        "\tsampled_pwm = K.switch(tf.convert_to_tensor(K.learning_phase(), dtype = tf.int32), st_sampled_softmax(flat_pwm), st_hardmax_softmax(flat_pwm))\n",
        " \n",
        "sample_func = sample_pwm\n",
        "\n",
        "validation_sample_mode = 'max'\n",
        "\n",
        "#if validation_sample_mode == 'sample' :\n",
        "#  sample_func = sample_pwm_only\n",
        "\n",
        "sampled_pwm_1 = Lambda(sample_func, name='pwm_sampler_1')(pwm_logits_1)\n",
        "sampled_pwm_2 = Lambda(sample_func, name='pwm_sampler_2')(pwm_logits_2)\n",
        "sampled_onehot_mask = onehot_mask"
      ],
      "metadata": {
        "id": "DRPyKVGdG1C-"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nt_probs = tf.nn.softmax(flat_pwm)\n",
        "onehot_dim = flat_pwm.get_shape().as_list()[1]\n",
        "\n",
        "#tf.compat.v1.distributions.Multinomial(flat_pwm, 1)\n",
        "\n",
        "#tf.multinomial(flat_pwm, 1)\n",
        "#tf.random.categorical(flat_pwm, 1)\n",
        "#tf.squeeze(tf.random.categorical(flat_pwm, 1), 1)\n",
        "tf.one_hot(tf.squeeze(tf.random.categorical(flat_pwm, 1), 1), onehot_dim, 1.0, 0.0)\n",
        "\n",
        "#sampled_onehot = tf.one_hot(tf.squeeze(tf.compat.v1.distributions.Multinomial(flat_pwm, 1), 1), onehot_dim, 1.0, 0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAvIRD7-Ocdc",
        "outputId": "bdc769c7-825a-4242-fcd6-96c985647f1b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(7380, 4) dtype=float32 (created by layer 'tf.one_hot_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_sequences = K.shape(pwm_logits_1)[0]\n",
        "seq_length = K.shape(pwm_logits_1)[1]\n",
        "\t\n",
        "flat_pwm = K.reshape(pwm_logits_1, (n_sequences * seq_length, 4))\n",
        "#sampled_pwm = K.switch(K.learning_phase(), st_sampled_softmax(flat_pwm), st_hardmax_softmax(flat_pwm))\n",
        "sampled_pwm = K.switch(tf.convert_to_tensor(K.learning_phase(), dtype = tf.int32), st_sampled_softmax(flat_pwm), st_hardmax_softmax(flat_pwm))"
      ],
      "metadata": {
        "id": "vpD7XmckYcQu"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_onehot_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTh3yvClbME5",
        "outputId": "f8a38114-261e-4293-cb5b-af4894884b5e"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(36, 205, 4, 1) dtype=float32 (created by layer 'reshape_7')>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator_model = Model(\n",
        "\tinputs=[\n",
        "\t\tsequence_class_input\n",
        "\t] + generator_inputs,\n",
        "\toutputs=[\n",
        "\t\tsequence_class,\n",
        "\t\tpwm_logits_1,\n",
        "\t\tpwm_logits_2,\n",
        "\t\tpwm_1,\n",
        "\t\tpwm_2,\n",
        "\t\tsampled_pwm_1,\n",
        "\t\tsampled_pwm_2,\n",
        "    onehot_mask,\n",
        "    sampled_onehot_mask\n",
        "\t] + extra_outputs\n",
        ")"
      ],
      "metadata": {
        "id": "uuoI2CPabdzR"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_sequence_templates(generator, sequence_templates) :\n",
        "\n",
        "\tembedding_templates = []\n",
        "\tembedding_masks = []\n",
        "\n",
        "\tfor k in range(len(sequence_templates)) :\n",
        "\t\tsequence_template = sequence_templates[k]\n",
        "\t\tonehot_template = iso.OneHotEncoder(seq_length=len(sequence_template))(sequence_template).reshape((len(sequence_template), 4, 1))\n",
        "\t\t\n",
        "\t\tfor j in range(len(sequence_template)) :\n",
        "\t\t\tif sequence_template[j] not in ['N', 'X'] :\n",
        "\t\t\t\tnt_ix = np.argmax(onehot_template[j, :, 0])\n",
        "\t\t\t\tonehot_template[j, :, :] = -4.0\n",
        "\t\t\t\tonehot_template[j, nt_ix, :] = 10.0\n",
        "\t\t\telif sequence_template[j] == 'X' :\n",
        "\t\t\t\tonehot_template[j, :, :] = -1.0\n",
        "\n",
        "\t\tonehot_mask = np.zeros((len(sequence_template), 4, 1))\n",
        "\t\tfor j in range(len(sequence_template)) :\n",
        "\t\t\tif sequence_template[j] == 'N' :\n",
        "\t\t\t\tonehot_mask[j, :, :] = 1.0\n",
        "\t\t\n",
        "\t\tembedding_templates.append(onehot_template.reshape(1, -1))\n",
        "\t\tembedding_masks.append(onehot_mask.reshape(1, -1))\n",
        "\n",
        "\tembedding_templates = np.concatenate(embedding_templates, axis=0)\n",
        "\tembedding_masks = np.concatenate(embedding_masks, axis=0)\n",
        "\n",
        "\tgenerator.get_layer('template_dense').set_weights([embedding_templates])\n",
        "\tgenerator.get_layer('template_dense').trainable = False\n",
        "\n",
        "\tgenerator.get_layer('mask_dense').set_weights([embedding_masks])\n",
        "\tgenerator.get_layer('mask_dense').trainable = False\n",
        "\n",
        "initialize_sequence_templates(generator_model, sequence_templates)"
      ],
      "metadata": {
        "id": "L_i2wDUpeq9C"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for generator_layer in generator_model.layers :\n",
        "  generator_layer.trainable = False\n",
        "  if 'policy' in generator_layer.name :\n",
        "    generator_layer.trainable = True"
      ],
      "metadata": {
        "id": "xuzY1vpYfdTV"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anneal_pwm_logits=False\n",
        "\n",
        "if anneal_pwm_logits :\n",
        "\treturn 'genesis_generator', generator_model, anneal_temp\n",
        "return 'genesis_generator', generator_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "ux5FfrYug__u",
        "outputId": "d1c5ca94-6da6-4f41-dc06-3bfe24314b79"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-114-890e101a6ed0>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    return 'genesis_generator', generator_model, anneal_temp\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhQgaIdmt9KA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}