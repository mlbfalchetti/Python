{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt3uKMHFFe+2S8rOBqqstY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlbfalchetti/Python/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(batch_size, seq_length, load_generator_function, n_classes=1, n_samples=None, sequence_templates=None, batch_normalize_pwm=False, anneal_pwm_logits=False, validation_sample_mode='max', supply_inputs=False) :\n",
        "\n",
        "\tuse_samples = True\n",
        "\tif n_samples is None :\n",
        "\t\tuse_samples = False\n",
        "\t\tn_samples = 1\n",
        "\n",
        "\tsequence_class_input, sequence_class = None, None\n",
        "\t#Seed class input for all dense/embedding layers\n",
        "\tif not supply_inputs :\n",
        "\t\tsequence_class_input = Input(tensor=K.ones((batch_size, 1)), dtype='int32', name='sequence_class_seed')\n",
        "\t\tsequence_class = Lambda(lambda inp: K.cast(K.round(inp * K.random_uniform((batch_size, 1), minval=-0.4999, maxval=n_classes-0.5001)), dtype='int32'), name='lambda_rand_sequence_class')(sequence_class_input)\n",
        "\telse :\n",
        "\t\tsequence_class_input = Input(batch_shape=(batch_size, 1), dtype='int32', name='sequence_class_seed')\n",
        "\t\tsequence_class = Lambda(lambda inp: inp, name='lambda_rand_sequence_class')(sequence_class_input)\n",
        "\n",
        "\n",
        "\t#Get generated policy pwm logits (non-masked)\n",
        "\tgenerator_inputs, [raw_logits_1, raw_logits_2], extra_outputs = load_generator_function(batch_size, sequence_class, n_classes=n_classes, seq_length=seq_length, supply_inputs=supply_inputs)\n",
        "\n",
        "\treshape_layer = Reshape((seq_length, 4, 1))\n",
        "\t\n",
        "\tonehot_template_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='template_dense')\n",
        "\tonehot_mask_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='ones', name='mask_dense')\n",
        "\t\n",
        "\tonehot_template = reshape_layer(onehot_template_dense(sequence_class))\n",
        "\tonehot_mask = reshape_layer(onehot_mask_dense(sequence_class))\n",
        "\n",
        "\n",
        "\n",
        "\t#Initialize Templating and Masking Lambda layer\n",
        "\tmasking_layer = Lambda(mask_pwm, output_shape = (seq_length, 4, 1), name='masking_layer')\n",
        "\n",
        "\t#Batch Normalize PWM Logits\n",
        "\tif batch_normalize_pwm :\n",
        "\t\traw_logit_batch_norm = BatchNormalization(name='policy_raw_logit_batch_norm')\n",
        "\t\traw_logits_1 = raw_logit_batch_norm(raw_logits_1)\n",
        "\t\traw_logits_2 = raw_logit_batch_norm(raw_logits_2)\n",
        "\t\n",
        "\t#Add Template and Multiply Mask\n",
        "\tpwm_logits_1 = masking_layer([raw_logits_1, onehot_template, onehot_mask])\n",
        "\tpwm_logits_2 = masking_layer([raw_logits_2, onehot_template, onehot_mask])\n",
        "\t\n",
        "\t#Compute PWMs (Nucleotide-wise Softmax)\n",
        "\tpwm_1 = Softmax(axis=-2, name='pwm_1')(pwm_logits_1)\n",
        "\tpwm_2 = Softmax(axis=-2, name='pwm_2')(pwm_logits_2)\n",
        "\t\n",
        "\tanneal_temp = None\n",
        "\tif anneal_pwm_logits :\n",
        "\t\tanneal_temp = K.variable(1.0)\n",
        "\t\t\n",
        "\t\tinterpolated_pwm_1 = Lambda(lambda x: (1. - anneal_temp) * x + anneal_temp * 0.25)(pwm_1)\n",
        "\t\tinterpolated_pwm_2 = Lambda(lambda x: (1. - anneal_temp) * x + anneal_temp * 0.25)(pwm_2)\n",
        "\t\t\n",
        "\t\tpwm_logits_1 = Lambda(lambda x: K.log(x / (1. - x)))(interpolated_pwm_1)\n",
        "\t\tpwm_logits_2 = Lambda(lambda x: K.log(x / (1. - x)))(interpolated_pwm_2)\n",
        "\t\n",
        "\t#Sample proper One-hot coded sequences from PWMs\n",
        "\tsampled_pwm_1, sampled_pwm_2, sampled_onehot_mask = None, None, None\n",
        "\n",
        "\tsample_func = sample_pwm\n",
        "\tif validation_sample_mode == 'sample' :\n",
        "\t\tsample_func = sample_pwm_only\n",
        "\n",
        "\t#Optionally tile each PWM to sample from and create sample axis\n",
        "\tif use_samples :\n",
        "\t\tpwm_logits_upsampled_1 = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]))(pwm_logits_1)\n",
        "\t\tpwm_logits_upsampled_2 = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]))(pwm_logits_2)\n",
        "\t\tsampled_onehot_mask = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]))(onehot_mask)\n",
        "\n",
        "\t\tsampled_pwm_1 = Lambda(sample_func, name='pwm_sampler_1')(pwm_logits_upsampled_1)\n",
        "\t\t#sampled_pwm_1 = Lambda(lambda x: K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)))(sampled_pwm_1)\n",
        "\t\tsampled_pwm_1 = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)), (1, 0, 2, 3, 4)))(sampled_pwm_1)\n",
        "\n",
        "\t\tsampled_pwm_2 = Lambda(sample_func, name='pwm_sampler_2')(pwm_logits_upsampled_2)\n",
        "\t\t#sampled_pwm_2 = Lambda(lambda x: K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)))(sampled_pwm_2)\n",
        "\t\tsampled_pwm_2 = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)), (1, 0, 2, 3, 4)))(sampled_pwm_2)\n",
        "\n",
        "\t\t\n",
        "\t\t#sampled_onehot_mask = Lambda(lambda x: K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)), (1, 0, 2, 3, 4))(sampled_onehot_mask)\n",
        "\t\tsampled_onehot_mask = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, seq_length, 4, 1)), (1, 0, 2, 3, 4)))(sampled_onehot_mask)\n",
        "\n",
        "\telse :\n",
        "\t\tsampled_pwm_1 = Lambda(sample_func, name='pwm_sampler_1')(pwm_logits_1)\n",
        "\t\tsampled_pwm_2 = Lambda(sample_func, name='pwm_sampler_2')(pwm_logits_2)\n",
        "\t\tsampled_onehot_mask = onehot_mask\n",
        "\t\n",
        "\t\n",
        "\tgenerator_model = Model(\n",
        "\t\tinputs=[\n",
        "\t\t\tsequence_class_input\n",
        "\t\t] + generator_inputs,\n",
        "\t\toutputs=[\n",
        "\t\t\tsequence_class,\n",
        "\t\t\tpwm_logits_1,\n",
        "\t\t\tpwm_logits_2,\n",
        "\t\t\tpwm_1,\n",
        "\t\t\tpwm_2,\n",
        "\t\t\tsampled_pwm_1,\n",
        "\t\t\tsampled_pwm_2\n",
        "\n",
        "\t\t\t,onehot_mask\n",
        "\t\t\t,sampled_onehot_mask\n",
        "\t\t] + extra_outputs\n",
        "\t)\n",
        "\n",
        "\tif sequence_templates is not None :\n",
        "\t\tinitialize_sequence_templates(generator_model, sequence_templates)\n",
        "\n",
        "\t#Lock all generator layers except policy layers\n",
        "\tfor generator_layer in generator_model.layers :\n",
        "\t\tgenerator_layer.trainable = False\n",
        "\t\t\n",
        "\t\tif 'policy' in generator_layer.name :\n",
        "\t\t\tgenerator_layer.trainable = True\n",
        "\n",
        "\tif anneal_pwm_logits :\n",
        "\t\treturn 'genesis_generator', generator_model, anneal_temp\n",
        "\treturn 'genesis_generator', generator_model"
      ],
      "metadata": {
        "id": "kF-DGY5ews5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}